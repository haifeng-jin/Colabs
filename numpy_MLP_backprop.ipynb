{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqEJL9cp/xugoPPPSqlOCG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haifeng-jin/Colabs/blob/main/numpy_MLP_backprop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def initialize_parameters(input_dim, hidden_dim, output_dim):\n",
        "    params = {\n",
        "        'W1': np.random.randn(input_dim, hidden_dim).astype(\"float32\") * 0.01,\n",
        "        'b1': np.zeros((1, hidden_dim)).astype(\"float32\"),\n",
        "        'W2': np.random.randn(hidden_dim, output_dim).astype(\"float32\") * 0.01,\n",
        "        'b2': np.zeros((1, output_dim)).astype(\"float32\")\n",
        "    }\n",
        "    return params\n",
        "\n",
        "# Example usage\n",
        "input_dim = 10  # Number of input features\n",
        "hidden_dim = 64  # Number of neurons in the hidden layer\n",
        "output_dim = 10  # Number of output classes\n",
        "num_samples = 5  # Number of samples\n",
        "\n",
        "# Dummy data: features, and their true classes\n",
        "X_dummy = np.random.randn(num_samples, input_dim).astype(\"float32\")\n",
        "\n",
        "def one_hot_encode(labels, num_classes):\n",
        "    # Create an array of zeros with shape (number of labels, number of classes)\n",
        "    one_hot_encoded = np.zeros((labels.size, num_classes))\n",
        "    # Set the appropriate elements to one\n",
        "    one_hot_encoded[np.arange(labels.size), labels] = 1\n",
        "    return one_hot_encoded\n",
        "\n",
        "\n",
        "# Generate random labels between 0 and 9 for 10 classes\n",
        "y_dummy_labels = np.random.randint(0, 10, size=num_samples)\n",
        "\n",
        "# One-hot encode these labels\n",
        "y_dummy = one_hot_encode(y_dummy_labels, 10)\n",
        "\n",
        "init_params = initialize_parameters(input_dim, hidden_dim, output_dim)"
      ],
      "metadata": {
        "id": "fm9SdUc20vdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "EnHYEdLczKS-"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def softmax(x):\n",
        "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
        "\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    # # Ensure numerical stability and avoid log(0) by adding a small constant\n",
        "    epsilon = 1e-12\n",
        "    # Clip predictions to avoid log(0)\n",
        "    y_pred = np.clip(y_pred, epsilon, 1. - epsilon)\n",
        "    # Compute the cross-entropy loss\n",
        "    loss = -np.sum(y_true * np.log(y_pred)) / y_true.shape[0]\n",
        "    return loss\n",
        "\n",
        "def forward_pass(X, params):\n",
        "    Z1 = np.dot(X, params['W1']) + params['b1']\n",
        "    A1 = relu(Z1)\n",
        "    Z2 = np.dot(A1, params['W2']) + params['b2']\n",
        "    A2 = softmax(Z2)\n",
        "    cache = (X, Z1, A1, Z2, A2)\n",
        "    return A2, cache\n",
        "\n",
        "def backward_pass(y_true, cache, params):\n",
        "    X, Z1, A1, Z2, A2 = cache\n",
        "    num_samples = y_true.shape[0]\n",
        "\n",
        "    def relu_derivative(inputs, gradients):  # dL/dx, y=relu(x), gradients are dL/dy\n",
        "        return gradients * (inputs > 0).astype(float)\n",
        "\n",
        "    def weight_derivative(inputs, gradients):  # dL/dW, y=Wx+b, gradients are dL/dy\n",
        "        return inputs.T.dot(gradients)\n",
        "\n",
        "    def bias_derivative(gradients): # dL/db, y=Wx+b, gradients are dL/dy\n",
        "        return np.sum(gradients, axis=0, keepdims=True)\n",
        "\n",
        "    dZ2 = (A2 - y_true) / num_samples\n",
        "    dW2 = weight_derivative(inputs=A1, gradients=dZ2)\n",
        "    db2 = bias_derivative(gradients=dZ2)\n",
        "\n",
        "    dA1 = dZ2.dot(params['W2'].T)\n",
        "    dZ1 = relu_derivative(inputs=Z1, gradients=dA1)\n",
        "    dW1 = weight_derivative(inputs=X, gradients=dZ1)\n",
        "    db1 = bias_derivative(gradients=dZ1)\n",
        "\n",
        "    # print(f\"dZ2 shape: {dZ2.shape}\")\n",
        "    # print(f\"dW2 shape: {dW2.shape}\")\n",
        "    # print(f\"db2 shape: {db2.shape}\")\n",
        "    # print(f\"dA1 shape: {dA1.shape}\")\n",
        "    # print(f\"dZ1 shape: {dZ1.shape}\")\n",
        "    # print(f\"dW1 shape: {dW1.shape}\")\n",
        "    # print(f\"db1 shape: {db1.shape}\")\n",
        "\n",
        "    grads = {\n",
        "        'W1': dW1,\n",
        "        'b1': db1,\n",
        "        'W2': dW2,\n",
        "        'b2': db2\n",
        "    }\n",
        "    return grads\n",
        "\n",
        "def update_parameters(params, gradients, learning_rate=0.1):\n",
        "    params['W1'] -= learning_rate * gradients['W1']\n",
        "    params['b1'] -= learning_rate * gradients['b1']\n",
        "    params['W2'] -= learning_rate * gradients['W2']\n",
        "    params['b2'] -= learning_rate * gradients['b2']\n",
        "    return params\n",
        "\n",
        "\n",
        "params = copy.deepcopy(init_params)\n",
        "# Train for a few epochs\n",
        "output, cache = forward_pass(X_dummy, params)\n",
        "loss = cross_entropy_loss(y_dummy, output)\n",
        "gradients = backward_pass(y_dummy, cache, params)\n",
        "params = update_parameters(params, gradients)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, params):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.layer2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "        self.layer1.weight = nn.Parameter(torch.from_numpy(np.transpose(params[\"W1\"])))\n",
        "        self.layer2.weight = nn.Parameter(torch.from_numpy(np.transpose(params[\"W2\"])))\n",
        "\n",
        "        self.layer1.bias = nn.Parameter(torch.from_numpy(params[\"b1\"]))\n",
        "        self.layer2.bias = nn.Parameter(torch.from_numpy(params[\"b2\"]))\n",
        "        self.temp_output = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer2(x)\n",
        "        # Softmax is applied in the loss\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "arnASDcs0jJU"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run():\n",
        "    params = copy.deepcopy(init_params)\n",
        "    # Train for a few epochs\n",
        "    logs = []\n",
        "    for epoch in range(10):\n",
        "        output, cache = forward_pass(X_dummy, params)\n",
        "        loss = cross_entropy_loss(y_dummy, output)\n",
        "        gradients = backward_pass(y_dummy, cache, params)\n",
        "        params = update_parameters(params, gradients)\n",
        "        logs.append(loss)\n",
        "    print(logs)\n",
        "    return logs\n",
        "\n",
        "\n",
        "def run_torch():\n",
        "    # Create the model\n",
        "    model = MLP(input_dim, hidden_dim, output_dim, copy.deepcopy(init_params))\n",
        "\n",
        "    # Dummy data: 5 samples, each with 10 features\n",
        "    X_dummy_torch = torch.from_numpy(X_dummy)\n",
        "    y_dummy_torch = torch.from_numpy(y_dummy)  # True classes\n",
        "\n",
        "    # Loss function and optimizer\n",
        "\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    logs = []\n",
        "    # Train for a few epochs\n",
        "    for epoch in range(10):\n",
        "        # Forward pass\n",
        "        logits = model(X_dummy_torch)\n",
        "        loss = criterion(logits, y_dummy_torch)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        logs.append(loss.item())\n",
        "\n",
        "    print(logs)\n",
        "    return logs\n",
        "\n",
        "\n",
        "def test():\n",
        "    np.testing.assert_allclose(run(), run_torch())\n",
        "\n",
        "test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOJ8rwG6Ru7C",
        "outputId": "f4ae51a6-c79b-4633-97d8-f3ea24848cf6"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2.301618242263794, 2.274173402786255, 2.247321891784668, 2.220949649810791, 2.195030164718628, 2.1695616245269775, 2.144471597671509, 2.1196786403656005, 2.0951594352722167, 2.0708685874938966]\n",
            "[2.3016183376312256, 2.274173450469971, 2.2473219871520995, 2.2209496974945067, 2.1950303077697755, 2.1695616245269775, 2.1444715023040772, 2.1196785449981688, 2.0951594352722167, 2.0708685874938966]\n"
          ]
        }
      ]
    }
  ]
}